# ==============================================
# âš™ï¸ AI ê¸°ë°˜ ì¥ì•  ëŒ€ì‘ ìƒí™©ì°½ ì–´ì‹œìŠ¤í„´íŠ¸ (RAG + Azure Search)
# ==============================================
import os
import streamlit as st
from dotenv import load_dotenv
from pathlib import Path
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
import json
import warnings
import re
warnings.filterwarnings("ignore", category=RuntimeWarning)

# ---------------------------------------------------------------------
# í™˜ê²½ ì„¤ì •
# ---------------------------------------------------------------------
load_dotenv()

# ğŸ”¹ OpenAI ì„¤ì •
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT")

# ğŸ”¹ Azure Search ì„¤ì •
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_INDEX = os.getenv("AZURE_SEARCH_INDEX")

# ğŸ”¹ Embedding ì„¤ì •
AZURE_EMBEDDING_MODEL = os.getenv("AZURE_EMBEDDING_MODEL")
AZURE_EMBEDDING_DEPLOYMENT = os.getenv("AZURE_EMBEDDING_DEPLOYMENT")
AZURE_EMBEDDING_ENDPOINT = os.getenv("AZURE_EMBEDDING_ENDPOINT")

# í…œí”Œë¦¿ íŒŒì¼ ë¡œë“œ
BASE_DIR = Path(__file__).resolve().parent
data_dir = BASE_DIR / "data"
template_path = data_dir / "incident_template.txt"

if not template_path.exists():
    st.error(f"í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {template_path}")
    st.stop()

with template_path.open("r", encoding="utf-8") as f:
    incident_template = f.read()

# ğŸ”§ ë””ë²„ê¹…ìš© í—¬í¼ í•¨ìˆ˜
def debug_log(title, value):
    print(f"\n===== ğŸ” {title} =====")
    try:
        if isinstance(value, dict):
            print(json.dumps(value, indent=2, ensure_ascii=False))
        else:
            print(str(value)[:1000])  # ë„ˆë¬´ ê¸¸ë©´ 1000ìê¹Œì§€ë§Œ í‘œì‹œ
    except Exception as e:
        print(f"(âš ï¸ ë””ë²„ê¹… ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e})")
    print("========================\n")

st.set_page_config(page_title="AI ê¸°ë°˜ ì¥ì•  ëŒ€ì‘ ìƒí™©ì°½ ì–´ì‹œìŠ¤í„´íŠ¸", layout="wide")

# ğŸ”§ ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• (ChatGPT ëŠë‚Œ)
st.markdown("""
<style>
    .stChatInputContainer {
        position: fixed;
        bottom: 0;
        left: 0;
        width: 100%;
        background-color: #ffffff;
        border-top: 1px solid #ddd;
        padding: 0.5rem 1rem;
    }
    .stChatMessage {
        max-width: 80%;
        padding: 0.75rem 1rem;
        border-radius: 0.8rem;
        margin-bottom: 0.5rem;
    }
    .stChatMessage[data-testid="stChatMessage-user"] {
        background-color: #DCF8C6;
        margin-left: auto;
    }
    .stChatMessage[data-testid="stChatMessage-assistant"] {
        background-color: #F1F0F0;
        margin-right: auto;
    }
</style>
""", unsafe_allow_html=True)

st.title("âš™ï¸ AI ê¸°ë°˜ ì¥ì•  ëŒ€ì‘ ìƒí™©ì°½ ì–´ì‹œìŠ¤í„´íŠ¸")
st.markdown("ì‹œìŠ¤í…œ ì¥ì•  ë°œìƒ ì‹œ, AIê°€ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•´ ì¤‘ê°„ë³´ê³ ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.")
st.markdown("---")

# LangChain êµ¬ì„± (RAG)
try:
    # 1ï¸âƒ£ Embedding ìƒì„±
    embeddings = AzureOpenAIEmbeddings(
        azure_endpoint=AZURE_EMBEDDING_ENDPOINT,
        azure_deployment=AZURE_EMBEDDING_DEPLOYMENT,
        api_key=AZURE_OPENAI_API_KEY,
        model=AZURE_EMBEDDING_MODEL
    )
    print("ğŸ”§ AZURE_EMBEDDING_MODEL:", os.getenv("AZURE_EMBEDDING_MODEL"))
    print("ğŸ”§ AZURE_EMBEDDING_DEPLOYMENT:", os.getenv("AZURE_EMBEDDING_DEPLOYMENT"))

    # 2ï¸âƒ£ Azure AI Search ì—°ê²°
    vector_store = AzureSearch(
        azure_search_endpoint=AZURE_SEARCH_ENDPOINT,
        azure_search_key=AZURE_SEARCH_KEY,
        index_name=AZURE_SEARCH_INDEX,
        embedding_function=embeddings,
        vector_field="content_vector",
        text_field="content"
    )

    print("ğŸ”— í˜„ì¬ ì—°ê²°ëœ Azure ì¸ë±ìŠ¤:", AZURE_SEARCH_INDEX)
    print("ğŸ”— í˜„ì¬ ì—°ê²°ëœ Azure ì—”ë“œí¬ì¸íŠ¸:", AZURE_SEARCH_ENDPOINT)

    print("ğŸ” AzureSearch í…ŒìŠ¤íŠ¸ ì¤‘...")
    

    # ğŸ“„ Retriever ìƒì„±
    retriever = vector_store.as_retriever(
        search_type="hybrid",
        k=5
    )

    # 3ï¸âƒ£ LLM ì´ˆê¸°í™”
    llm = AzureChatOpenAI(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_API_KEY,
        api_version=AZURE_OPENAI_API_VERSION,
        deployment_name=AZURE_OPENAI_DEPLOYMENT,
        temperature=0.2
    )

    # 4ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ì •ì˜ (í…œí”Œë¦¿ + ë¬¸ì„œ ê¸°ë°˜)
    prompt = ChatPromptTemplate.from_template("""
    ë„ˆëŠ” ì‹œìŠ¤í…œ ì¥ì•  ëŒ€ì‘ì„ ì§€ì›í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
    ì•„ë˜ ë§¤ë‰´ì–¼ê³¼ í…œí”Œë¦¿ì„ ì°¸ê³ í•´ ìƒí™©ì°½ ëŒ€í™”ë¥¼ ì¤‘ê°„ë³´ê³  í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.

    [ì¥ì•  ëŒ€ì‘ í…œí”Œë¦¿]
    {template}

    [ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©]
    {context}

    [ìƒí™©ì°½ ë©”ì‹œì§€]
    {question}
    """)

    # 5ï¸âƒ£ ì²´ì¸ ìƒì„±
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
    )
except Exception as e:
    st.error(f"LangChain ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()

# ---------------------------------------------------------------------
# ì„¸ì…˜ ê´€ë¦¬
# ---------------------------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    
# âœ… rerun ë°©ì§€ìš©: Streamlitì´ rerunë  ë•Œ messages ìœ ì§€
messages = st.session_state.get("messages", [])

# for msg in st.session_state.messages:
#     st.chat_message(msg["role"]).write(msg["content"])

# âœ… ì±„íŒ… ì˜ì—­ (ìŠ¤í¬ë¡¤ ê°€ëŠ¥)
chat_container = st.container()

# âœ… ì…ë ¥ì°½ í•˜ë‹¨ ê³ ì •
user_input = st.chat_input("ğŸ—¨ ìƒí™©ì°½ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ERP ë¡œê·¸ì¸ ì˜¤ë¥˜, DB ì ‘ì† ë¶ˆê°€, ë³µêµ¬ ì§„í–‰ ì¤‘...)")

# âœ… ì´ì „ ëŒ€í™” ì¶œë ¥
with chat_container:
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"], unsafe_allow_html=True)

# âœ… ìƒˆ ë©”ì‹œì§€ ì…ë ¥ ì²˜ë¦¬
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€
    st.chat_message("user").markdown(user_input.replace("\n", "<br>"), unsafe_allow_html=True)
    st.session_state["messages"].append({"role": "user", "content": user_input})

    # âœ… ì´ì „ ëŒ€í™” í¬í•¨
    chat_history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state["messages"]])
    query_with_context = f"{chat_history}\nuser: {user_input}"

    with st.spinner("AIê°€ ì¤‘ê°„ë³´ê³ ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            rag_input = {"question": query_with_context, "template": incident_template}

            retriever_output = retriever.invoke(query_with_context)
            context_texts = "\n".join([d.page_content for d in retriever_output]) if retriever_output else ""

            prompt_input = {
                "context": context_texts,
                "question": query_with_context,
                "template": incident_template
            }

            # ğŸ”¹ LLM í˜¸ì¶œ
            raw_llm_response = llm.invoke(prompt.format(**prompt_input))
            ai_output = str(raw_llm_response)

            # ğŸ”¹ content ë¶€ë¶„ë§Œ ì¶”ì¶œ
            match = re.search(r"content='(.*?)' additional_kwargs=", ai_output, re.DOTALL)
            if match:
                content_text = match.group(1).strip()
            else:
                content_text = ai_output.strip()

            # ğŸ”¹ \n â†’ ì¤„ë°”ê¿ˆ ë³µì›
            content_text = content_text.replace("\\n", "\n")

            # ğŸ”¹ ëŒ€í™” ì¶”ê°€ ë° ì¶œë ¥
            st.session_state.messages.append({"role": "assistant", "content": content_text})
            st.chat_message("assistant").markdown(content_text)

        except Exception as e:
            st.error(f"RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")





